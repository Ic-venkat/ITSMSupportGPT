{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model =\"llama2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://www.langchain.com/langsmith\"),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "csvloader = CSVLoader(file_path=\"test.csv\")\n",
    "csvdoc = csvloader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = all_splits,\n",
    "    embedding = embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\":6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Continuously track qualitative characteristics of any live application, and spot issues in real-time with LangSmith monitoring.Add observability to strengthen your applications.Request a demoSign UpOne-click deployEasily deploy your applications with hosted LangServe. Get built in parallelization, fallbacks, batch, streaming, and async support, coupled with familiar LangSmith visibility and a built-in playground.Explore LangServe\\n\\n\\nMonitor cost, latency, quality.See what’s happening with your production application, so you can take action when needed or rest assured while your chains and agents do the hard work.Go to Docs\\n\\n\\nUser feedback collection\\n\\n\\n\\n\\n\\n\\n\\nAdvanced filtering\\n\\n\\n\\n\\n\\n\\n\\nOnline \\u2028auto-evaluation\\n\\n\\n\\n\\n\\n\\n\\nCost tracking\\n\\n\\n\\n\\n\\n\\n\\nInspect anomalies and errors\\n\\n\\n\\n\\n\\n\\n\\nSpot latency spikes\\n\\n\\n\\n\\n\\n\\n\\nUser feedback collectionAdvanced filteringOnline \\u2028auto-evaluationCost trackingInspect anomalies and errorsSpot latency spikes', metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 3070, 'title': 'LangSmith'}),\n",
       " Document(page_content='Inspect anomalies and errors\\n\\n\\n\\n\\n\\n\\n\\nSpot latency spikes\\n\\n\\n\\n\\n\\n\\n\\nUser feedback collectionAdvanced filteringOnline \\u2028auto-evaluationCost trackingInspect anomalies and errorsSpot latency spikes\\n\\nLangSmith turns LLM \\u2028\"magic\"\\xa0into enterprise-ready applications.Got a question?Can I use LangSmith if I don’t use LangChain?\\n\\nYes! Many companies who don’t build with LangChain use LangSmith. You can log traces to LangSmith via the Python SDK, the TypeScript SDK, or the API. See here for more information.I can’t have data leave my environment. Can I self-host LangSmith?\\n\\nYes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.How easy is it to start using LangSmith if I use LangChain?', metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 3812, 'title': 'LangSmith'}),\n",
       " Document(page_content=\"Getting started is as easy as setting three environment variables in your LangChain code. When you use the LangSmith SDK, there’s a callback handler to collect traces and send them to your LangSmith Organization. All your trace steps will be formatted automatically, so there’s virtually no set up cost.My application isn’t written in Python or TypeScript. Will LangSmith be helpful?\\n\\nYes, we have an API that allows you to programmatically interact with every feature LangSmith has to offer, including logging traces, creating datasets, and running evals. See documentation for more detail.Where is LangSmith data stored?\\n\\nTraces are stored in GCP us-central-1. Organizations' traces are logically separated from each other in a Clickhouse database and encrypted in transit and at rest.What information is contained in LangSmith traces?\", metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 4659, 'title': 'LangSmith'}),\n",
       " Document(page_content='LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nLangChainLangSmithLangServeMethods\\n\\nRetrievalAgentsEvaluationResources\\n\\nDeveloper ResourcesTemplatesIntegrationsLangChain QuickstartLangChain.js QuickstartCompany ResourcesBlogCase StudiesUse Case InspirationPartnersDocs\\n\\nLangChain DocsLangSmith DocsPricingCompany\\n\\nAboutCareersSign UpGet your LLM app from prototype to productionAn all-in-one developer platform for every step of the LLM-powered application\\xa0lifecycle, whether you’re building with LangChain or not.Sign upRequest a demo\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications.100K+Users signed up\\n\\n\\n\\n\\n\\n\\n\\n200M+Traces logged\\n\\n\\n\\n\\n\\n\\n\\n20K+Monthly active teams', metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 0, 'title': 'LangSmith'}),\n",
       " Document(page_content='We will not train on your data, and you own all rights to your data. See LangSmith Terms\\xa0of\\xa0Service for more information.How much does LangSmith cost?\\n\\nSee our pricing page for more information, and find a plan that works for you.Ready to start shipping \\u2028reliable GenAI apps faster?LangChain and LangSmith are critical parts of the reference \\u2028architecture to get you from prototype to production.Request a demoSign UpProductsLangChainLangSmithLangServeAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsTemplatesLangChain QuickstartLangChain.js QuickstartCompanyAboutBlogTwitterDiscordLinkedInYouTubeSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service', metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 6376, 'title': 'LangSmith'}),\n",
       " Document(page_content='Collaborate with teammates to get app behavior just right.Building LLM-powered applications requires a close partnership between\\xa0developers and subject matter experts.Go to Docs\\n\\n\\nTracesEasily share a chain trace with colleagues, clients, or end users, bringing explainability to anyone with the shared link.001HubUse LangSmith Hub to craft, version, and comment on prompts. No\\xa0engineering experience required.002Annotation QueuesTry out LangSmith Annotation Queues to add human labels and feedback on traces.003DatasetsEasily collect examples, and construct datasets from production data or existing sources. Datasets can be used for evaluations, few-shot prompting, and even fine-tuning.004Test & Evaluate: Measure quality over large test suites.Layer in human feedback on runs or use AI-assisted evaluation, with\\xa0off-the-shelf and custom evaluators that can check for relevance,\\xa0correctness, harmfulness, insensitivity, and more.Evaluation\\n\\n\\n001Dataset Construction', metadata={'description': 'Get your LLM app from prototype to production.', 'language': 'en', 'source': 'https://www.langchain.com/langsmith', 'start_index': 1479, 'title': 'LangSmith'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"What is LangGraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith is a platform for developing, collaborating, testing, deploying, and monitoring LLM applications. It provides a unified DevOps experience for users, allowing them to log traces, create datasets, run evals, and monitor their applications in real-time. LangSmith collects traces from various sources, including the LangChain and other applications, and stores them in GCP us-central-1. The platform is designed to provide full visibility into what's happening in users' applications, allowing them to debug and improve their applications easily.\\n\\nTo answer your question, LangSmith does not train on the data that users send it. Instead, it collects and stores traces for monitoring and debugging purposes. Additionally, LangSmith allows users to specify the percentage of traces they want to send to the platform, starting from SDK version 0.0.84 (Python) and 0.0.64 (JS). This feature enables users to control the amount of data they want to send to LangSmith while still benefiting from the platform's features.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is LangSmith used for \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run(\"print(1+1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create the tool to pass to an agent\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'PythonREPL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tools\n\u001b[0;32m      3\u001b[0m tool_names \u001b[38;5;241m=\u001b[39m [python_repl]\n\u001b[1;32m----> 4\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\agents\\load_tools.py:621\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, allow_dangerous_tools, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _handle_callbacks(\n\u001b[0;32m    618\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m), callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m    619\u001b[0m )\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m tool_names:\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDANGEROUS_TOOLS\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_tools:\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    623\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a dangerous tool. You cannot use it without opting in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    624\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting allow_dangerous_tools to True. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m         )\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'PythonREPL'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tool_names = [python_repl]\n",
    "tools = load_tools(tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool, DuckDuckGoSearchRun\n",
    "\n",
    "# llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "math_tool = Tool.from_function(\n",
    "    func=llm.run,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Useful for when you are asked to perform math calculations\"\n",
    ")\n",
    "\n",
    "tools = [search_tool, math_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
